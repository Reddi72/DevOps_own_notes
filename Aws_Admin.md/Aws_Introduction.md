# Cloud Computing:
it is on-demand and delivery of IT resoureces and applications via the internet with pay-as-you go pricing.
# Cloud computing Types:
1. Private cloud:
2. Public cloud:
3. Hybrid cloud:
# Cloud computing Services:
1. On premises: in this model  customer can manage all resources like applications,data,runtime,middleware,o/s,virtualization,servers,storage,networking. by itâ€™s own

2. Infrastructure as a service:In this service customer can manage only applications, data, runtime, middleware, o/s. remaining services cloud vender provides to u like virtualization, servers, storage, networking
#Ex:Virtual Machines or AWS EC2, Storage or Networking

3. Platform as a service:in this services customer can manage only two services like applications and data. Remaining all services like runtime, middleware, o/s, virtualization, servers, storage, networking are take and care by cloud vender.
#Ex: Elastic Beanstalk or Lambda from AWS, WebApps, Functions or Azure SQL DB from Azure, Cloud SQL DB from Google Cloud, or Oracle Database Cloud Service from Oracle Cloud

4. Software as a service:in this model all resources provide by cloud vender.
#Ex: Microsoft Office 365, Oracle ERP/HCM Cloud, SalesForce, Gmail, or Dropbox

# AWS Core Infratructure and Servcies #
# Traditional Infrastructure      #Amazon Web Services
1. Security:                      1. Security
 1. Firewalls                       1.Security Groups 
 2. ACLs                            2. NACLs
 3. Administrators                  3. AWS IAM
2. Networking                     2. Networking
 1. Router                          1. ELB
 2. Network Pipeline                2. VPC
 3. Switch               
3. Servers                        3. Servers
 1. On-premises Servers             1.AMI-->Ec2 Instnces
4. Storage & Database             4. Storage & Database
 1. DAS                             1. Amazon EBS
 2. SAN                             2. Amazon EFS
 3. NAS                             3. Amazon S3
 4. RDBMS                           4. Amazon RDS
## AWS Global Infrastructure #
# Regions:
AWS regions(25) are separate geographical areas, like the US-West 1 (North California) and Asia South (Mumbai).
It conistes atleast 2 Avialability zones.

# Avialabilty Zones:
availability zones [81]are the areas that are present inside the regions. These are generally isolated zones that can replicate themselves whenever required.
Clusters of data centres.

## AWS Services ##
1. IAM User:
AWS-->Security Identity & Compliance-->IAM
IAM enables you to manage access to AWS services and resources securely.You use IAM to control who is authencated (signed in) and authorized (has permissions) to use resoureces.
#uses:
-- Fine-grained access control to AWS resoureces.
-- Multi-factor authentcation.
-- Integrate with your corporate directory.
-- Manage access control for mobile applications with Web Identity Providers.
# Important Terms
1. Users
2. Groups
3. Roles
# How it works ?
User Dave--->Policy Administrator access-->all aws services access
Group Admins-->user susan-->PowerUserAccess-->all aws services access
Role-->role third party access-->policy S3 Storage Readonly access-->s3 access.

# IAM
You can change alias name what u want
goto IAM user dashboard-->Account alias-->create --> give ur own name(prasad)-->if this name avialable it creates url.
here u can see Sign url change with your name.
Share this url who want add this aws account.
# Security Status.
1. Delete your access keys
2. Activate MFA(Multi factor authencation) on your root account.
add mfa-->Activate mfa-->choose ur mfa assign device-->virtual mfa device--->setup virtual device-->Install a compatible app on your mobile device or computer-->download below any app in ur mobile
# For Android:	Twilio Authy Authenticator, Duo Mobile, LastPass Authenticator, Microsoft Authenticator, Google Authenticator, Symantec VIP
# For iOS:	Twilio Authy Authenticator, Duo Mobile, LastPass Authenticator, Microsoft Authenticator, Google Authenticator, Symantec VIP
-->then open that app-->scan the QR code which one u can see in ur computer screen-->then u should be see in ur mobile two codes enter those keys in ur computer-->Assign MFA.  
3. create individual IAM users
Users-->add user-->user name(Prasad)-->Select AWS access type-->1.Access key - Programmatic access 2.Password - AWS Management Console access-->select one or two-->Console password--->Autogenerated password-->Require password reset(user can change after first login)--->Next permissions-->Set permissions-->Attach existing policies directly-->S3 access(here choose permissions which u want to user)-->s3 full access-->Next:tags-->Key(team)-->Value(Developement)--->Next-->Review-->create user--->download .csv-->here shows all user login details.

Then aws logout and login to as IAM user and Sign.
Root user email address: 917790459966
IAM user name: Prasad
Password: 0UQQuL9j@6(6bo+  Then Login then set ur new password.  then logged into ur aws .here u can access s3 only.
4. Use groups to assign permissions:
# in realtime u can create somany users and manages that by useing groups
create a multiple users
users--->add user-->name(reddi)-->Select AWS access type-->1.Access key - Programmatic access 2.Password - AWS Management Console access-->select one or two-->Console password--->Autogenerated password-->Require password reset(user can change after first login)--->Next permissions-->Set permissions-->add user to group-->Next:tags-->Key(team)-->Value(Developement)--->Next-->Review-->create user--->download .csv.
# Create Group
User groups-->create group-->name(family)-->select all users -->select user policy-->create user group.

5. Apply an IAM password policy.
Account settings-->change password policy-->Select your account password policy requirements-->save changes.
# Role:
An IAM role is an identity you can create that has specific permissions with credentials that are valid for short durations. Roles can be assumed by entities that you trust.
 Role-->Create role-->Trusted entity type-->aws service-->ec2-->Permissions policies-->s3 bucket-->next-->tags-->key(Team)-->value(development)-->Role name(Ec2@s3 admin)-->create role.
==================================================================================================================


## *** Autoscaleing *** ##
Auto Scaling groups are collections of Amazon EC2 instances that enable automatic scaling and fleet management features. These features help you maintain the health and availability of your applications.

# Load balancer
Elastic Load Balancing scales your load balancer capacity automatically in response to changes in incoming traffic.
# Create load balancer:
Ec2-->load balancing-->load balncer-->create load balncer-->select load balancer-->Load balancer name(new_lb)-->VPC-->choose ur vpc-->mappings-->choose minimum 2 avalability zones-->select security groups-->port 80-->create load balancer.
1. Application Load Balancer: Choose an Application Load Balancer when you need a flexible feature set for your applications with HTTP and HTTPS traffic. Operating at the request level, Application Load Balancers provide advanced routing and visibility features targeted at application architectures, including microservices and containers.
2. Network Load Balancer: Choose a Network Load Balancer when you need ultra-high performance, TLS offloading at scale, centralized certificate deployment, support for UDP, and static IP addresses for your applications. Operating at the connection level, Network Load Balancers are capable of handling millions of requests per second securely while maintaining ultra-low latencies.
3. Gateway Load Balancer: Choose a Gateway Load Balancer when you need to deploy and manage a fleet of third-party virtual appliances that support GENEVE. These appliances enable you to improve security, compliance, and policy controls.

# Auto Scaling group:
An Auto Scaling group is a collection of Amazon EC2 instances that are treated as a logical unit.Setting different minimum and maximum capacity values forms the bounds of the group, which allows the group to scale as the load on your application spikes higher or lower, based on demand.let Amazon EC2 Auto Scaling automatically add and remove capacity to meet changes in demand.

# launch configuration:
Creating a launch configuration allows you to create a saved instance configuration for EC2 Auto Scaling.
After creating a launch configuration and associating it with an Auto Scaling group.
# Create launch configuration:
ec2-->auto scaling-->lanuch configuration-->Launch configuration name(new_lc)-->Amazon machine image (AMI)-->choose ami-->choose Instance type(medium)-->Additional configuration-->IAM instance profile-->Monitoring -->EBS volumes-->Security groups-->Rules-->Key pair-->create launch configuration.

# Launch template
-->Create launch template-->Launch template name-->Amazon Machine Image--->Key pair name-->Security groups-->Advanced details-->lauch template

# Create Auto Scaling group:
Ec2-->Auto scaling-->Auto scaling groups-->Create Auto Scaling group-->group name(new_group)-->choose template-->group size(1)-->subnet-->Advanced details-->loadbalancing-->choose ur loadbalancer-->health check-->use scale policies to adjust the capacity of this group-->scale between minimum instances 1 and maximum instances4-->sample scaling policy-->increase group size-->add new alarm-->send notification to mailid-->cpu utilization>=70%-->consecutive period(5 minutes)-->create alarm-->take the action-->add-->1 instance when cpu utilization is above 70%.-->instance need 60 sec.
Decrese group size-->add new alarm-->send notification to mailid-->cpu utilization<=10%-->consecutive period(5 minutes)-->create alarm-->take the action-->remove-->1 instance when cpu utilization is less 10%.-->instance need 60 sec.-->create.
=========================================================
## *** Cloud Watch *** ##
# Rule:
Amazon Event bridge--->Rules-->create rule-->Name-->Rule type(1.Rule with an event pattern 2.Schedule)-->next-->Schedule-->cron *****-->targets-->ec2 creates snapshot API call-->volume id-->configure.

# Alarms:
Cloud watch-->all alarms-->create alarm-->metric-->select metric-->ec2-->cpu utilization-->selct metric-->Period-->5m-->Whenever CPUUtilization is-->Greater/Equal
>= threshold-->thanâ€¦Define the threshold value(85)-->Datapoints to alarm(1-1)-->add notification-->Alarm state trigger-->In alarm-->Create new topic-->Email-->create topic-->alarm name(cpu)-->review and create.
# Billing alarm:
cloud watch-->alarms-->billing-->create alarm-->Conditions-->Whenever EstimatedCharges is.Greater/Equal>= threshold 40 usd dollars-->next-->email-->create.
# Createing Dashboard:
cloud watch-->dashboard-->name-->create dash board-->add widget-->choose template(ec2)-->create.
====================================================
## AWS Lambda #
is compute service where you can upload your code and create a lambda function. AWS lambda takes care of provisioning and manageing the servers that you use to run the code. You don't worry about operting systems,patching,scaling..etc.

## Load balanceing #
User--->Webserver--->Load balancer
                          |
                |"""|"""""|""""|"""|"""|"""|
                   EC2 Ec2   Ec2  Ec2 Ec2 Ec2 Ec2
here load balancer distribute load throughwards ec2 as per end user demand.
# Create Load balancer #
AWS-->Ec2-->Load balancing-->Load balncers-->create load balancer-->classic loadbalncer-->Load Balancer name-->select vpc-->Listener Configuration-->port-->next-->Assign Security Groups-->choose security group of webserver and select it-->next-->Configure Health Check-->Ping Protocol-->HTTP
Ping Port-->80-->Ping Path-->/index.html-->Advanced Details-->Advanced Details-->Response Timeout(5sec)
-->Interval(30 seconds)-->Unhealthy threshold(2)-->Healthy threshold(10)-->next-->Add EC2 Instances-->select instances-->Next & Add Tags-->Review and create..
# Configure Health Check
Your load balancer will automatically perform health checks on your EC2 instances and only route traffic to instances that pass the health check. If an instance fails the health check, it is automatically removed from the load balancer. Customize the health check to meet your specific needs.
======
Now create security groups with allow httpd 80 port add this security group to instnces.
create 2 instnces and attch this load balancer to this instancs.
login to 2 servers thorugh putty/gitbash
#sudo su -
#yum install httpd -y
#cd /var/www/html
#echo "<html><h1>This is my home page from aws cloud2<h1><html>" > index.html
#systemctl start httpd
#chconfig httpd on
 Then
goto loadbalncer-->instances-->status-->inservice.
then -->descrption-->copy dns address and searcgh in google here shows web page and refresh it shows 2diffrent output for 

# Auto Scaleing #
# Placement group:
it is a logical grouping of instances within a single availability zone. useing placement groups enables applications to paritcipate in a low-latency, 10 gbps network. placement groups are recommend for applications that low network latency,high network thorughpoint or both.
1. A placement group can't span multiple avialable zones.
2. The name you specify for a placement  group must be unique with in your aws account.
3. only certain types of instncs can be launched in a placement group(compute optimizes,GPU,memory optimized,storage optimized)
4. Aws recommend homogenous instances within placement groups
5. u can't merge placement groups.
6. u can't move an existing instance into a placement group.you can create an ami from existing instance and then launch a new instance from the ami into placement group.

Ec2-->Network nd security--->placement groups-->create placement group-->name(First_plcgrp)-->Placement strategy-->cluster-->  

# EFS:
Amazon Elastic File System (Amazon EFS) provides a simple, scalable, elastic file system for general purpose workloads for use with AWS Cloud services and on-premises resources.

aws--->EFS-->create file system-->Name-->choose vpc-->storage-->select avialability zones-->security groups-->create.
go to ec2 and create instnces and attch same security groups to all and create.
login ur servers thorugh putty..
#sudo yum install amazon-efs-utils -y
#mkdir efs
#cd /
#ls  --L here shows efs
#mount -t nfs4 -o nfserver=4.1,rsize=1048576,wsize=1048567,hard,time0=600,retrsns....etc
#cd efs
#cat file1 -->server1
#cat file2 -->server2
above 2 files shows in both servers.
==================================================================================================================
## EC2 ###
EC2 is short for Elastic Compute Cloud .it provides scalable computing capacity and eliminates the need to invest in hardware . 
You can use Amazon EC2 to launch as many or as few virtual servers as needed, configure security and networking, and manage storage. It can scale up or down to handle changes in requirements, reducing the need to forecast traffic.
EC2 is a virtual machine that represents a physical server for you to deploy your applications. Instead of purchasing your own hardware and connecting it to a network, Amazon gives you nearly unlimited virtual machines to run your applications while they take care of the hardware.
## Create EC2 Instance ##
From the Amazon EC2 console dashboard, choose Launch instance.
1.	Step 1: Choose an Amazon Machine Image (AMI) ... it is a template that contains the "software" configuration like amzn linux,Ubuntu,centos,macos,redhat,suse
2.	Step 2: Choose an Instance Type. ...like t2.micro,t2.large,t3.micro,a1.large,c1 large,xle.32xlarge,gpuâ€¦
3.	Step 3: Configure Instance Details. ...like no if instance ,subnet..
4.	Step 4: Add Storage. ...
5.	Step 5: Add Tags. ...
6.	Step 6: Configure Security Group. ...inbound rule
7.	Step 7: Review Instance Launch and Select Key Pair.
.pem key downloaded then converted into .ppk using puttygen tool. then login through putty.

copy ur ec2 instance public ip then open ur putty 
Hostip(34.224.98.138)-->Apperence-->change-->font style(bold)-->size(16)-->ok-->ssh-->click on +-->auth-->browse-->selct ur ppk key-->open-->login as-->ec2-user-->press enter.

# Types of Instances #
1. On-Demand instance:
you pay for compute capacity by per hour or per second depending on which instnces you run.
No longer-term commintments.
No upfront payments are needed.
# Recommands:
Users that prefer the low cost and flexibility of Amazon EC2 without any up-front payment or long-term commitment

* Applications with short-term, spiky, or unpredictable workloads that cannot be interrupted

* Applications being developed or tested on Amazon EC2 for the first time


2. Reserved instance:
provide you with a capacity reservation, and offer significant disount on hourly charge for an instance with 1year or 3 years
1 year / 3 years agreement or commitment with AWS.

3. Spot instances:
enble you to bid whatever price you want for instance capacity,provideing greater savings if your application have flexible start and end times.
There is no speed and performance guarantee from AWS
* If unused instance available then AWS provide offer to customer/users
* Based on biding:
    AWS price: $0.0043 /h
    Your Max:  $1 /h
Recommnds to:
4. Dedicated instance:
it is a physical ec2 server dedicated for your use.dedicated hosts can help you reduce costs by allowing you to use existing server-bound software licences,including windows server,SQL server and SUse,linux server.    
======================================================
1. Genaral Purpose:A1,T3,T2,M5,M5a,M4,T3a
2. Compute Optimized:C5,C5n,C4
3. Memory Optimized: R5,R5a,R4,X1,X1e,High memory,Z1D
4. Acclerated Computing:P2,P3,G3,F1
5. Storage Optimized: H1,I3,D2
===========================================
## Block Base Storage(EBS) ##
Elastic Block Storage (EBS) allows you to create storage volumes and attach them to Ec2 instances.once attached, you can create a file sytem on top of these volumes run a database or use them in any other way you would use a block device. EBS volumes are placed in a speicfic avilability zone,where they are automatically replicated to protct you from the failuere of a single component.
# EBS types #
1. Genaral purpose SSD(GP2):
genaral purpose balancs both price and performance.
Ratio of 3 IOPS per GB with up to 10,000 IOPS and the ability to burst up to 3000 IOPS for extended periods of time for volumes under 1gib
2. Provisioned IOPS SSD:
Designed for I/O intensive applications such as large relational or NoSql databases.
use if you need more than 10,000 IOPS
can provison up to 20,000 IOPS per volume
3. Through out Optimized HDD(ST1):
big data 
Data warehouses
Log processing 
cannot be a boot volume
4. Cold HDD (Sc1):
Lowest cost storage for infrequently accessed workloads 
file server 
cannot be a boot volume.
5. Magnetic(Standard)
Lowest cost per gigbyate of all EBS volumes types that is bootable. magntic voulmes are ideal for workloads where data is accessed infrequintly,and applications where lowest storage cost is important.
=======================================================
## Windows server creation ##
aws-->ec2-->launch instance-->ami-->windows server 2019-->choose instance type-->t2.micro-->configure instance-->defult-->add storgae-->default-->security group-->windows-sg-->rds-->port 3389-->next-->select exiting keypair-->review and launch. 

selct windows server-->connect-->rdp client-->get new password-->upload ur key-->decrypt password-->copy the password & dns address and userid(Public DNS
 ec2-3-95-18-1.compute-1.amazonaws.com
User name
 Administrator
Password
 1fwEiuqw&*FOp%53H)b&THesGC4=qt%L)-->username(adminstrator)-->open ur remote desktop-->paste dns url-->connect-->enter credentials-->user name and password-->ok-->yes.
======================================================
## Instance metadat #
#curl http://169.254.169.254/latest/meta-data
here u can see all meta of ur instnce
#curl http://169.254.169.254/latest/meta-data/ami-id
here shows ur ami id
#curl http://169.254.169.254/latest/meta-data/public-ipv4
here shows ur public ip address

## Linux basics #
# Linux machines 
1. RPM based                         2. Debain based
-Centos                               -Ubuntu
-RHEL (Redhat Enterpriselinux)
-Amazon linux
#yum update -y                      #apt-get update -y
#yum install httpd -y          #apt-get install apache -y
 
# Security groups #
Setup of firewall rules.
Inbound rules control the incoming traffic to your instance, and outbound rules control the outgoing traffic from your instance.

Aws-ec2-security groups-creare group-->inbound rules-->add-->http-->80-->allow.

# EBS
chekc ur ebs voulume in instance
#lsbls --->here shows all block storage.
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   8G  0 disk
â””â”€xvda1 202:1    0   8G  0 part    /
# Add somemore ebs volume to ur server
Aws-->ec2-->elastic block storage-->volumes-->create volume-->Volume settings-->Size(3gb)-->Availability Zone-->ur instance avialabilty(east1b)-->create volume.

voluems u can see ur voulme and it's status is avialble it means it is not in use.
## Attach volume to instance
select volume-->actions-->Instance-->choose instance-->Device name-->it's default(/dev/sdf)-->attach.

go back to putty and see ur volumes
# lsblk --> here 2 voulmes root & attached volume
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   8G  0 disk
â””â”€xvda1 202:1    0   8G  0 part /
xvdf    202:80   0   3G  0 disk
 now check any file systm attached to this volume

# file -s /dev/xvda1 -->root volume hav files
/dev/xvda1: SGI XFS filesystem data (blksz 4096, inosz 512, v2 dirs)

#file -s /dev/xvdf
/dev/xvdf: data -->it means no files 
# mkfs -t ext4 /dev/xvdf
file system is created
# cd /
# mkdir reddi/
# mount dev/xvdf reddi/
#  lsblk
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   8G  0 disk
â””â”€xvda1 202:1    0   8G  0 part /
xvdf    202:80   0   3G  0 disk /reddi
# cd reddi
# touch file1
#  cd /
# umount dev/xvdf 
mount will be removed
# cd reddi
# ls --> here not shows any files
# lsblk
lsblk
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   8G  0 disk
â””â”€xvda1 202:1    0   8G  0 part /
xvdf    202:80   0   3G  0 disk

goto aws -->volumes-->select volume-->detach.
refresh it then status shows avialble
now backup this-->select this volume-->actions-->create snapshot-->descrption-->new snphshot-->create snapshot(backup).

If u want to increae ur volume then
volumes-->select volume-->actions-->modify-->add size-->create.
## Snapshot #
it backup of ur volume
here u can use this to create volume/image/copy/modify permissions.
1. copy 
select-->snapshot-->actions-->copy-->Destination region-->choose ur destion region-->enacrypt-->copy.
2. Modify permissiions:it helps for share snapshot to other aws accounts.
selct-->snapshot-->actions-->modify permissions-->private-->aws account number-->add permissions-->save
3. Create volume:
select snapshot-->actions-->create volume-->volume type-->genaral purpose ssd-->size(3gb)-->avialability zone(same as instance avblty zone)-->create volume.
============================================
# Boot strap server #
aws-->ec2-->lanuch instance-->name(bootstrap_server)-->ami-->amazon linux-->key pair-->security group with 80 port-->advanced-->user data-->
#!/bin/bash
yum update -y
yum install httpd -y
cd var/www/html
echo "<html><h1>this is bootstarp server<h1><html>">index.html
systemctl start httpd
chconfig httpd on
-->review and lanuch.

then goto google-->publicip-->search-->here shows ur website.
========================================================
## Using IAM Role with Ec2 & AWS command line Tools #
AWs-->IAM-->user-->add user-->username(reddi)-->access type-->tik programmatic access-->next-->add user to group-->s3admin group-->add tags-->key(name)-->value(developer)-->next-->create user-->download.csv.

Ec2-->launch instance-->choose instance-->configure insance-->add storage-->add tags-->configure security group-->choose security group-->lauch

# sudo su -
# aws configure 
AWS Access Key ID [None]: AKIA5LMETBA7LSLUTJG2
AWS Secret Access Key [None]: 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
Default region name [None]: us-east-1
Default output format [None]: ENTER
# aws s3 ls
# cat file1
# aws s3 cp file1 s3://awsbucket/file1 --dryrun
-- for testing
# aws s3 cp file1 s3://awsbucket/file1
ur file1 uploaded to s3 bucket -->for signle file
# aws s3 cp s3://awsbucket/file1 file1
file1 downloaded from s3 bucket.
# aws s3 sync /home/ec2-user/ s3://awsbucket/
here total folder uploaded to s3 bucket
# cd ~
# ls -la
.  ..  .aws  .bash_logout  .bash_profile  .bashrc  .cshrc  .ssh  .tcshrc
# cd .aws
# ls
config  credentials
# cat credentials -->here shows ur key and secrt key
[default]
aws_access_key_id = AKIA5LMETBA7LSLUTJG2
aws_secret_access_key = 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
# rm -rf credentials
now u can't access s3 services 
============================================
AWS-->IAM-->role-->create role-->aws service-->ec2-->next-->serach-->s3 full accesss-->next-->next--name of the role(ec2se_fullacces role)-->create.
go back to ec2-->select instance-->actions-->security->attach replac IAM role-->select role-->apply.

go back to putty
# aws s3 ls
here shows all buckets
======================================================
## Amazon Machine Image(AMI) ##
it is a template that contains the software configuration like amzn linux,Ubuntu,centos,macos,redhat,suse.

Aws-->lanuch new instance.
now install LAMP stack:linux+apache+mysql+php
1.Security patches
2.Linux server + httpd + php+sdk + php-mysql
login to putty
# sudo su -
# yum update -y
# yum install httpd -y
# yum install php -y
# yum intall php-mysql -y
# php -r "copy('https://getcomposer.org/installer', 'composer-setup.php');"
# php -r "if (hash_file('sha384', 'composer-setup.php') === '55ce33d7678c5a611085589f1f3ddf8b3c52d662cd01d4ba75c0ee0459970c2200a51f492d557530c71c15d8dba01eae') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;"
# php composer-setup.php
# php -r "unlink('composer-setup.php');"
# composer require aws/aws-sdk-php
# ls
composer.json  composer.lock composer.phar
========================

Now Take it AMI
goto aws-->select this instance-->actions-->Image and Template-->create image-->Image name(Lamp_stack)-->create image.

See ur ami images:
ec2-->images-->amis-->here u can see ur ami.
selcet ami-->actions-->copy-->choose destination region-->copy ami.
use this ami u can launch,spot requset,deregister,register new ami,copy ami,modify image permissions.
========================================================
## Root Device Volumes & EBS vs Instance Store #
1. cannot stop the ec2 instance with instance store volumes(only reboot & terminate)
2. cannot view the volume from AWS store 
3. any data stored on the instance store volume is lost (if m/c is freezed or with problems)
4. instance store volumes are otherwise called as empherel storage
5. EBS volumes are otherwise called as perisistent storage.
# Raid, Volumes & Snapshots
RAID: Redundant array of indepedent disks
 RAID 0: stripped,no redundancy,good performance
 RAID 1: mirrored,redundancy
 RAID 5: good for reads,bad for writes,AWS doesnot recommend ever putting RAID 5's on EBS
 RAID 10: stripped mirroed good redundancy good performance.

Go to AWS
Aws-->ec2-->lanuch instances-->choose ami-->windows 2019-->choose instance-->t2.micro-->next-->add storage-->add new 3 volumes 2gb+2gb+2gb-->next-->review and launch.
selcet instance-->connect-->get password-->key pair path-->browse ur keypair-->decrypt-->copy the dns,userid,password.
open ur remote desktop connection-->paste ur dns-->connect-->enter credentials-->userid-->password-->ok-->yes..
computer management-->c drive-->here u can see 30gb and 2gb,2gb,2gb-->select and right click-->online-->keep 3 volumes in online-->selct right click and initilize it 3volumes-->selct right clck -->new starred volume-->next-->next-->assign the following drive letter-->d-->next-->perform a quick format-->next-->finish.
u cans see top newvolume mounted.
=========================================================
 ### Simple Storage Service S3 #### Object Based Storage
S3 provides developers and IT teams with secure,durable,fast,highly scalable object storage.Amazon s3 is easy to use,with a simple web interface to store and retrieve any amount of data from anywhere on the web.
S3 is a place to store files
S3 is object based storage
the data is spread across multiple faciliteis and services.
# S3 basics # 
S3 is object based storage i.e. allows you to upload files
files are stored in buckets 
size of single file can be from 0 bytes to 5TB
there is unlimited storage
S3 is univesal namespace,that is name of the bucket must be unique globally.
read after write consistency for puts of new object
evently cosistency for overwrite puts and Deletes (can take some time to propagate).

AWS-->S3-->create bucket-->name(new_s3_bucket)-->region-->create.
select-->s3-->upload-->add files-->selct file-->upload.
select-->s3 bucket-->permissions-->public access settings-->edit-->save-->confirm.
select-->s3 bucket-->make public-->object url
## S3 url
http://s3-<regionName>.amazonaws.com/<bucketname>/<filename>
# S3 Object 
S3 is object based.object consits of the follwing.
- Key (This is simply the name of the object)
- Value (This is simply the data and is made up of sequence of bytes)
- Version id(important for versioning)
- Metadata (data about data)
- Access control lists.
- Amazon gurantee 99.99% avialability for the s3 platform
- amazon gurantee 99.99% durability for s3 (11*9's)
- tired storage management
- versioning
- life cycle management
- encrption
# Properties#
1. S3 classes
Standard
intellgent-tiering
Standard-1A
one-zone-1A
Glicier
Reduced Redudency
2. Encryption
 1. none --> means object not encrypted
 2. AES-256--> means encrypt ur data
 3. AWS-KMS--> means encrypt data & use a key for decrption
3. Metadat:
it shows meta data of s3 bucket like png/pdf/jpg
4. Tags:
for objects to search organize and manage access
5. Object lock:
prevent this object from being deleted.
it enable at creating of this object. u can't enable after creation s3 bucket
# Permissions #
access for object owner and Access for other aws accounts and public access.
# Select from #
u want store some log files of webserer in s3 buckets .
u can see that logs in format of csv/json/parquet
============
aws-->s3-->buckets-->click on bucket
here u can see 
1. Overview:
2. Properties:
 1. Versioning:keep multiple versions of an object in the same bucket.
 Enable vesrsioning
 add file-->select file-->upload-->it have specific version id.
 u can speicfic file useing this version id
 versions-->hide-->some one delete object.
 u can recover it through versionging like
shows versions-->here shows delete file-->here u can see deleted file with delete marker-->select it and actions-->delete marker-->now ur file will be recoverd
 2. Server access logging:
setup  access log records that provide details about access requests.
Enable logging-->target bucket-->choose bucket-->target prefix-->/logs-->save
overview-->create folder-->logs.
after sometime logs stored
 3. Static website hosting:
host a static website, which does not require server side technologies
click on it-->here u can see 3 options-->1.use this bucket to host a website 2.redirect requsest 3.disable website hosting.-->click on 1st-->index document-->index.html-->error document-->error.html-->copy the dns url-->save
goto notepad and write index.html file (<html><h1>this is my home page from aws s3 bucket from region-mumbia <h1><html>) and also write error.html(<html><h1>this is a error page go back to home page<h1><html>)
upload this files to ur s3 bucket
open s3-->upload-->select these file-->upload
goto google-->search url dns -->here shows ur html header.
 4. Object-level-logging:
cloud taril for recording activity-->choose existing cloud trail(or)create new trail.
 5. Default encryption:
    1. none --> means object not encrypted
    2. AES-256--> means encrypt ur data
    3. AWS-KMS--> means encrypt data & use a key for decrption
 6. object lock:prevent objects from being deleted.
 7. tags: for objects to search organize and manage access
 8. Transfer acceleraton: Enable fast,easy when specific events occur in ur bucket.
 enable-->want to compare ur data transfer speed by region-->here u can see speed.
 9. events:Receive notifications when specific events occur in ur bucket.
 add notification-->name(delete event)-->events-->permenately deleted-->prefix(folder)-->suffix(file)-->send to-->sns topic-->sns-->add sns topic arn-->sns-->topics-->create subscription-->protocal-->email-->end point-->email-->create subscrption
 10. Request pays: the requester (instead of the bucket owner)will pay for requests and data transfer.

3. Permissions: 
 1. Public access settings: use the s3 block public access settings to enforce that buckets don't allow public access to data. it have default all allows false. 
 2. Access control list: Access for your aws account root user.
 3. Bucket Policy:type or rewrite existing policy.
 policy genarator select type of policy-->s3 bucket policy-->add statements-->effect-->allow-->aws service-->amazon s3-->actions-->add statement.
 4. CORS configuration: cross-origin resource shareing (cors) defines way for client web application that are loaded in one domain interect with resources in a diffrent domain.
4. Management:
 1. lifecycle:
 2. Replication:
 add rule-->set source-->entire bucket-->replication cretirea-->next-->destinations bucket-->create new bucket-->bucket name()-->region()-->options-->change the storage class for the replicated object-->select storage class--> standard1A-->next-->iam role-->create new role-->rule name(myfirst_rule)-->enabled-->next-->save.
 3. Analytics:
 4. Metrics:
 5. Inventory: u can recieve a file of ur object inventory on a daily or weekly basis for the entire bucket or a shared.
 add new-->name-->select bucket-->frequsency-->daily/weekly-->output format-->csv-->object versions-->choose all versions/specific version-->optional fields-->size/last modified date-->encryption --->save.
 6. Add lifecycle rule-->enter rule name-->add filter to limit scope of prefix-->next-->storage class transition-->current version-->next-->storage class transition-->for current versions of object-->object creation-->transition to standard-1A after-->30-->next-->configure expiration-->current version-->expire current version of object-->after-->425days-->next.

=========================================================
## ROUTE53 ##
# DNS: 
if u have used the internet you have used DNS. DNS is used to convert human friendly domain names(such as http://example.com) into an internet protocol(IP) address (such as http://12.4.433.4).
IP addresses are used by computers to identify each other on the network. IP addess commonly come in 2 different forms ,IPv4,IPv6
# IPv4 vs IPv6:
The IPv4 space is a 32 bit field and has over 4 billion different addresses (4,294,967,296 to be precise).

The IPv6 was created to solve this depletion issue and has an addresses space of 128bits which in theory is
340,282,366,920,938,463,463,374,607,431,768,211,456 addresses  or 340 undecillion addresses
# Top level domains # 
if we look at common domains names such as google.com,bbc.co.uk etc.. u will notice a string of character separeated by dots(periods). the last word in a domain name represents the "top level domain"the  second word in a domain name(this is optional thorugh and depends on the domain name) like .com .edu .gov .co.uk .gov.uk .com.au
these top level domain names are controlled by the internet assigned numbers authority(IANA) in a root zone database which is essentially a database of all avilable top level domains.u can view this database by visiting.
http://www.iana.org/domains/root/db
# Domain Registars:
because all of the names in a given domain name have to be unique there needs to be a way to orgnise this all so that   domain names aren't duplicated. this is where domain registars come in.A register is an authority that can assign domain names directly under one or more top-level domains. theses domains are registered with InterNIC, a service of ICAN,which enforces uniqueness of domain names across the internet.each domain name becomes registred in a central database known as the whois database.
Popular domain registars include Godaddy.com, 123-reg.co.uk ..etc
# SOA records #
The name of the server that supplied the data for the zone.
The adminstrator of the zone
The current version of the data file
the default number of seconds for the time-to-live file on resource records.
# NS record:
NS stands for name server records   and are used by top level domain severs to direct traffic to the content DNS server which contains the authorative DNS records.
# A records:
An 'A'record is the fundmental type of DNS record and the 'A' in A record stands for 'Address'.The A record by a computer to translate the name of the domain to the IP address. for Exmple http://www.exmple.com   might point http://123.44.443.80
# TTL(Time To Live):
The length that a DNS record is cashed on either the Resloving server or the users own local PC is equal to the value of the "time to live" in seconds. the lower the time to live,the fast changes to DNS records take to propagate throughout the internet.
# CNAMES:
A canonical Name(CNAME) can be used to resolve one domain name to another. for example you may have a mobile website with the domain name http://facebook.com
that is used for when users browse to ur domain name on their mobile devices.you may also want the name http://fb.com to resolve to this same address. 
fb is canonical name facebook.com
# Alias records:
Alias records are used to map resource record sets in ur hosted zone to elastic load balancer, cloud front distributions, or S3 buckets that are configured as websites.
Alias records work like a CNAME record in that u can map one DNS name (www.facebook.com) to another 'target' DNS name (elb1234.elb.amazonaws.com)
key difference - A CNAME can't be used for naked domain names (zone apex). you can't have a CNAME for http://facebook.com. it must be either an A record or an alias.

# LAB #
# Flow Chart:
Domain Register-->Domain hosting-->NS records--> A record-->Alias Load balancer-->Ec2 instnace Webserver.

# Step1: Purchase Domain
Purchase the domain from "freenom.com" in this domain purchaser .ml domains are free.
freenom.com-->Search-->Reddi.ml-->checkout-->3 months free-->continue-->ask ur gmail for sign in-->enter ur mail-->verify link will sedn to ur mail-->verify it-->enter basic details-->continue-->purchased-->again login to freenom.com-->login-->services-->my domains-->reddi.ml-->manage domain-->management tools-->name servers
# Step2: Create a hosting in amazon route53
Aws-->Route53-->Hostedzones-->created hostzones-->domain name(reddi.ml)-->created hosted zone-->here u can see 2 records NS,SOA-->copy NS names-->goto freenom-->mydomains-->reddi.ml-->manage domain-->management tools-->name servers-->customize-->paste here NS names.

# Step3: Verify the following records-(SOA & NS)
# Step4: map NS records from Route 53 Hosted zone into place we have purchased Domain 
# Step5: Create an Ec2 instance
creat new instance for httpd webserver.
# Step6: Create a load balancer and map it with ec2 instance
Aws-->Ec2-->loadbalancing-->load blancer-->create loadbalancer-->classic loadbalancer-->name(mywebLB)-->next-->add webserver security group-->next-->configure health check-->ping path(/healthy.html)-->next add to Ec2instance-->select ur webserver-->add tags-->key(Teams)-->value(Development)-->review and create.
check ur loadbalancer useing DNS url search in google.
# Step7: Create an a record and map the naked domain with alias - ELB(load balancer)
aws-->Route53-->hosted zones-->select hosteszone-->create record-->alias on-->Route traffic to-->Alias to Application and classic loadbalancer-->region(us-east(n.viragia))-->load balancer-->select ur loadbalancer-->Routing policy-->simple-->create record.

# Step8: Create one or more A record with 'www' and map it with the existing hosted zone
aws-->Route53-->hosted zones-->select hosteszone-->create record-->name(www)-->alias on-->Route traffic to-->choose endpoint-->Alias to another record in this hosted zone-->choose record-->reddi.ml-->create record

goto google-->www.reddi.il.
# Step9: Test the hosting with naked domain/apex record and with full domain name.
========================================================
## Route53 Routing Policies ##
1. Simple:
           This is the defult routing policy when create
a new record set.This is most commonly used when u have a single resources that performs a given function for your domain,for example, one web server that servers content for the http://exmple.com website.
In this exmple Route53 will respond to DNS queries that are only in the record set(ie there is no intellgence built in to this response)
2. Weighted:
            this policies let you spilt your traffic
based on different weights assigned.
for exmple u can set 10% of ur traffic to go to US-EAST-1 and 90% to go EU-WEST1
3. Latency:
           it allows you to route ur traffice based on
the lowest network latency for ur end user(ie which region will give them the fastest response time)
to use latency-based routing you create a latency   resource record set for the amazon Ec2(or ELB) resource in each region that hosts ur website.when amazon route53 recieves a query for ur site,it selects the latency resource record set  for the region that gives the user the lowest latency. Route53 then responds with the value associated with resource record set.  
Ex: user from india access my website but i have hosted us-east1 and us-west1, Route53 decides which is the least and fastest  way to reach the server for enduser.
depending on latency as we see here 53milli seconds us-west1 server and 104ms for us-east1. then user automatically divert to the uswest1 in this latency based routing.

4. Failover:
            this polices are used when you want to crea
te an active/passive setup.
Ex:u may want ur primary site to be in US-West-1 and ur secondary DR site in US-East1.
Route53 will monitor the health of ur primary site using a health check.
A health check monitors the health of ur end points.

5. Geolocation:
               Lets choose where ur traffic will be sent 
based on the geographic location of ur users(ie the location from which DNS quires orginate).
Ex: u might want all quries from Europe to be routed to a fleet of Ec2 instnces that are specifically configured for ur Europen customers. These servers may have the local language of ur Europen customers and all prices are displyd i Euros.
=====================================================

### VPC (Virtual Private Cloud) ###
u can easily customize the network configuration for ur amazon Virtual Private cloud. 
Ex: u can create a public-facing subnet for ur webservers that has access to the internet, and place ur backend systems such as databases or application servers in a private-facing subnet with no internet access.u can leverage multiple layers of security,including security groups and network access control lists,to help control access to amazon EC2 instances in each subnet.

Additonally u can create hardware Virtual Private Network(VPN) connection between ur corporate datacenter and ur VPC and levarage the AWS cloud as an extension of ur corporate datacenter.
# What can u do with a VPC #
1. Launch instance into a subnet of ur choosing
2. Assign custtom IP address ranges in each subnet
3. Configure route tables between subnets
4. Create internet gateway and attach it to our vpc 
5. Much better security control over your AWs resources
6. Instance security group
7. Subnet network access control lists(ACLS)
# Default VPC vs Custom VPC # 
Default VPC is user friendly, allowing you to immediately deploy instances.
All subnets in default VPC have a route out to the internet.
Each EC2 instance has both a public and private IP address
If u delete the default VPC the only way to get it back is to contact aws.
# VPC Peering #
Allows u to connect one VPC with another via a direct network route using private ip addresses
Instances behave as if they were on the same private network
You can peer VPC's with other aws accounts as well as with other VPCsin the same account.
peering is in a star configuration, i.e 1 central VPC peers with 4 others.NO Transitive peering.
## VPC Lab ##
** PART-1 ***
Step1: Create VPC (Name of the VPC,CIDR Range,Tenancy)
Step2: Verify that the route table and NACL & default SG is automatically created.
Step3: Creating Subnets(Name of the subnet,VPC,AZ,CIDR range)
Step4: Create Internet gateway (Name of the IGW)
Step5: Attach the IGW to vpc that we created
Step6: Create a Internet Routetable & Attach the subnet & edit the route

** PART-2 **
NAT Instance (Network Address Translation):
Step1: Create security group for NAT(Name of the SG)
Step2: Update the Inbound rules of the security group to the private facing subnets (HTTP/HTTPS--10.0.2.0/24) 
Step3: Creat NAT instance
--> Choose AMI (select NAT)
--> Launch it in the vpc that we created & place this instance in public subnet & IP address Diable
--> Attach it to security group that we created in step1
Step4: Disable source/destination check on NAT instacne(Actions-->Networking-->Change Source/Destination Check)
Step5: Main Route table of the VPC that we created,edit routes (update destination and tag it to NAT instance)
Step6: Create a Elastic IP address and attach it to NAT instance.
NAT Gateway:
Step1: go to  VPC section, and then NAT gateway category. Create NAT gateway (Subnet-->public subnet, EIP allocation)
Step2: Edit main route table of the VPC edit routes(Update Destination and tag it to NAT Gateway).
# CIDR (Classless Inter-Domain Routing) #
10.0.0.0/16 means 65536 IP address
goledn formula:   32-16   16
                 2     = 2  =65536 ips
10.0.1.0/24: 256 ips  32-24   8
                     2      =2 =256- ips
In this 5 ips aws keep 5ips for future ref
256-1= 251 ips
10 .      0 .     1 .     150
(octet) (octet) (octet)  (octet)
0-255    0-255  0-255     0-255
# NACL(Network Access Control List):
- Rules are written in multiple of 100
- Since NACL is stateless, inbound rules shall be copied to the outbound as well.
- Rule with the lowest number gets evaluated first following ascending order there by
- you can associate 
    - one NACL --> Multiple subnets
    - one subnet --> one NACL
NACL has capability of allow/deny
Custom NACL: 
Ingress: ALL port/protocol deny
Outgress: All port /protocol.
=======================================================
1. Go to AWS console-->VPC-->create Vpc-->VPC and more-->vpc name(Reddi)-->cidr(10.0.0.0/16)-->create.

2. Verify that the route table and NACL & default SG is automatically created
goto-->route table-->here u can see route tables with reddi.
goto-->Network ACLs--> here u can see nacl created by reddi.
goto-->Route tables-->here u can see default routetable created.

3. goto-->subnets-->create subnet-->Subnet name(subnet1_public)-->Availability Zone(1a)-->IPv4 CIDR block(10.0.1.0/24)-->add new subnet-->Subnet name(subnet2_private)-->Availability Zone(1b)-->IPv4 CIDR block(10.0.2.0/24)-->create.

4. Internet gateways-->create-->name(Reddi-IGW)-->create.

5. attach this internetgateway to ur vpc.
select Reddi-IGW-->Actions-->Attach to vpc-->select ur Reddi-vpc-->attach

6. Route table-->Create route table-->Name(Reddi_Route2)-->VPC-->select Reddi vpc-->Create.

Route table-->select Reddi_Route2-->actions-->Edit subnet associations-->Available subnets-->slect subnet1-->save.

Routetable-->edit routes-->add route-->Destination(0.0.0.0/0)-->Target-->Internat gatway-->choose ur internatgateway-->save.
=======================================================

Launuch instance-->Name(vpc_server1)-->choose ami(amazon linux)-->Instance type(t2.micro)-->Key pair name(my_newkey)-->Network settings-->VPC (Reddi)-->Subnet-->choose reddi_pblicsubnet-->Auto-assign public IP-->enable-->next-->next-->review launch.

Launuch instance-->Name(vpc_server2)-->choose ami(amazon linux)-->Instance type(t2.micro)-->Key pair name(my_newkey)-->Network settings-->VPC (Reddi)-->Subnet-->choose reddi_privatesubnet-->Auto-assign public IP-->enable-->next-->next-->review launch.

go to putty-->login vpc_server1
# sudo su -
open ur ppk key in notepad and copy it.
# vi key.ppk
paste here ppk key
# amazon-linux-extras install epel
# yum install putty
# puttygen key.ppk -O private-openssh -o key.pem
# chmod 400 key.pem
# ssh -i key.pem ec2-user@10.0.2.80 (private ip of vpcserver2)
here connect ur vpcserver2 connected.
========================================================
1. Ec2-->Security Groups-->Create security group-->Security group name(NAT_Sg)-->Description(NAT_Sg)-->VPC(Reddi)-->
2. Inboundrules-->add rule-->http-->Source-->10.0.2.0/24-->Add rule-->https-->source-->10.0.2.0/24-->save.
3. Ec2-->Launch instance-->name(Nat_server)-->AMI-->browse-->Community AMIs-->Search-->Nat-->selct 1-->Key pair name-->Network settings-->VPC-->Reddi_vpc-->Subnet-->public_subnet-->select exiting Sg-->Nat_Sg-->Review & Launch.
4. Select NAT_server-->Actions-->Networking-->change source/destination-->stop-->save.
5. Vpc-->Route tables-->select Reddidefault routetable-->Edit routes-->add route-->0.0.0.0/0-->target-->Instance-->NAT_server-->save.
6. Ec2-->Network&Security-->Elastic Ips-->Allocate Elastic IP address-->Allocate.
delete NAT server.
======================================================
Putty-->log in server1 
#sudo su -
#ssh -i key.pem ec2-user@10.0.34.23
#yum update -y
#yum install httpd -y
#yum install php -y

VPC-->NAT gateway-->create gateway-->Name(My_Nat_gateway)-->Subnet-->select Reddi_publicsubnet-->Connectivity type-->Allocate Elastic IP-->create Nat gateway-->Edit route tables-->Reddi_default-->Edit routes-->destination-->0.0.0.0/0-->Target-->NAT gateway-->My_NAt gateway-->save routes.
Delete Nat GAtway and deallocate Elastic ip address.

VPC-->Network acl-->Create network ACL-->name(Custom_nacl)VPC-->select Reddi_vpc-->create.

Select-->Custom_nacl-->subnet associations-->Edit subnet associations-->slect subnet1-->edit.
outbound rules-->add rule-->100-->custom TCP rule-->port-->22-->destination(0.0.0.0/0)-->allow.
add rule-->port-->200-->custom tcp-->80-->0.0.0.0/0-->allow.
add rule-->300-->custom tcp-->port-->443-->0.0.0.0/0-->allow.

Inbound rule-->add rule-->100-->custom TCP rule-->port-->22-->destination(0.0.0.0/0)-->allow.
add rule-->port-->200-->custom tcp-->80-->0.0.0.0/0-->allow.
add rule-->300-->custom tcp-->port-->443-->0.0.0.0/0-->allow.
add rule-->400-->custom tcp-->port-->1024-65535-->0.0.0.0/0--->allow.






















